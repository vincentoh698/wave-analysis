{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d5f9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, roc_curve, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf9e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "arff_train = arff.loadarff('wave_2_classes_with_irrelevant_attributes.train.arff')\n",
    "train = pd.DataFrame(arff_train[0])\n",
    "\n",
    "arff_test = arff.loadarff('wave_2_classes_with_irrelevant_attributes.test.arff')\n",
    "test = pd.DataFrame(arff_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f363b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['classe'] = train['classe'].replace([train['classe'].unique()[0], train['classe'].unique()[1]],[0,1])\n",
    "test['classe'] = test['classe'].replace([test['classe'].unique()[0], test['classe'].unique()[1]],[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76eded39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>alea92</th>\n",
       "      <th>alea93</th>\n",
       "      <th>alea94</th>\n",
       "      <th>alea95</th>\n",
       "      <th>alea96</th>\n",
       "      <th>alea97</th>\n",
       "      <th>alea98</th>\n",
       "      <th>alea99</th>\n",
       "      <th>alea100</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.04</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3.63</td>\n",
       "      <td>4.59</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.76</td>\n",
       "      <td>4.58</td>\n",
       "      <td>...</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.73</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.82</td>\n",
       "      <td>2.93</td>\n",
       "      <td>5.55</td>\n",
       "      <td>3.48</td>\n",
       "      <td>5.65</td>\n",
       "      <td>5.01</td>\n",
       "      <td>3.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.57</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.32</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.84</td>\n",
       "      <td>3.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.99</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.07</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.71</td>\n",
       "      <td>6.98</td>\n",
       "      <td>3.87</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1.18</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.72</td>\n",
       "      <td>3.58</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.40</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.68</td>\n",
       "      <td>5.33</td>\n",
       "      <td>4.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-0.07</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.87</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.26</td>\n",
       "      <td>5.07</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2.04</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.84</td>\n",
       "      <td>4.37</td>\n",
       "      <td>6.50</td>\n",
       "      <td>5.47</td>\n",
       "      <td>3.67</td>\n",
       "      <td>2.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.59</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2.37</td>\n",
       "      <td>4.87</td>\n",
       "      <td>5.27</td>\n",
       "      <td>4.81</td>\n",
       "      <td>6.03</td>\n",
       "      <td>4.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-0.91</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.12</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.89</td>\n",
       "      <td>2.87</td>\n",
       "      <td>6.22</td>\n",
       "      <td>6.17</td>\n",
       "      <td>2.93</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1    v2    v3    v4    v5    v6    v7    v8    v9   v10  ...  alea92  \\\n",
       "0    -0.38 -0.72  1.30  3.04  2.10  3.63  4.59  3.55  0.76  4.58  ...    0.79   \n",
       "1    -0.33  0.04  0.83  0.70  3.26  1.73  3.48  3.52  1.45  1.91  ...    0.20   \n",
       "2     0.48  0.94  2.49  2.82  2.93  5.55  3.48  5.65  5.01  3.90  ...    0.42   \n",
       "3     0.57  1.15  1.71  2.45  0.60  2.32  2.29  1.74  1.84  3.16  ...    0.00   \n",
       "4    -1.99 -1.87  1.03  2.07  3.42  3.71  6.98  3.87  1.16  2.85  ...    0.02   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...     ...   \n",
       "9995  1.18  0.96  1.72  3.58  2.10  2.40  3.24  3.68  5.33  4.92  ...    0.11   \n",
       "9996 -0.07  1.59  0.87 -0.42  0.58  1.26  5.07  4.65  4.10  3.64  ...    0.77   \n",
       "9997  2.04 -0.79  2.46  1.08  2.84  4.37  6.50  5.47  3.67  2.29  ...    0.28   \n",
       "9998  0.59  1.54  0.81  1.09  2.37  4.87  5.27  4.81  6.03  4.29  ...    0.55   \n",
       "9999 -0.91  1.16  2.12  3.14  3.46  3.89  2.87  6.22  6.17  2.93  ...    0.40   \n",
       "\n",
       "      alea93  alea94  alea95  alea96  alea97  alea98  alea99  alea100  classe  \n",
       "0       0.20    0.22    0.21    0.42    0.76    0.95    0.86     0.07       0  \n",
       "1       0.86    0.83    0.95    0.48    0.96    0.20    0.73     0.71       0  \n",
       "2       0.77    0.94    0.51    0.15    0.89    0.62    0.31     0.21       0  \n",
       "3       0.51    0.29    0.38    0.07    0.83    0.18    0.82     0.76       0  \n",
       "4       0.48    0.15    0.05    0.26    0.72    0.03    0.18     0.84       0  \n",
       "...      ...     ...     ...     ...     ...     ...     ...      ...     ...  \n",
       "9995    0.52    0.98    0.65    0.82    0.83    0.69    0.47     0.25       1  \n",
       "9996    0.29    0.64    0.34    0.86    0.72    0.88    0.48     0.54       1  \n",
       "9997    0.82    0.13    0.75    0.71    0.82    0.63    0.76     0.88       1  \n",
       "9998    0.24    0.53    0.84    0.32    0.05    0.03    0.42     0.32       1  \n",
       "9999    0.79    0.86    0.33    0.63    0.28    0.98    0.56     0.63       1  \n",
       "\n",
       "[10000 rows x 122 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c88dde90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>alea92</th>\n",
       "      <th>alea93</th>\n",
       "      <th>alea94</th>\n",
       "      <th>alea95</th>\n",
       "      <th>alea96</th>\n",
       "      <th>alea97</th>\n",
       "      <th>alea98</th>\n",
       "      <th>alea99</th>\n",
       "      <th>alea100</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.14</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.42</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>1.27</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>-2.23</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.20</td>\n",
       "      <td>3.04</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.08</td>\n",
       "      <td>4.06</td>\n",
       "      <td>1.46</td>\n",
       "      <td>4.32</td>\n",
       "      <td>3.94</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.45</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-1.56</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>1.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23329</th>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.34</td>\n",
       "      <td>3.26</td>\n",
       "      <td>4.20</td>\n",
       "      <td>5.19</td>\n",
       "      <td>3.83</td>\n",
       "      <td>5.37</td>\n",
       "      <td>4.05</td>\n",
       "      <td>2.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23330</th>\n",
       "      <td>1.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.84</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.13</td>\n",
       "      <td>4.64</td>\n",
       "      <td>3.23</td>\n",
       "      <td>5.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23331</th>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.93</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23332</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.03</td>\n",
       "      <td>1.44</td>\n",
       "      <td>3.79</td>\n",
       "      <td>4.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23333</th>\n",
       "      <td>-1.56</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.32</td>\n",
       "      <td>1.54</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.28</td>\n",
       "      <td>4.28</td>\n",
       "      <td>5.19</td>\n",
       "      <td>3.44</td>\n",
       "      <td>5.23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23334 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v1    v2    v3    v4    v5    v6    v7    v8    v9   v10  ...  \\\n",
       "0     -1.14  1.21  0.42  0.83  1.08  1.42 -0.09  0.91  0.02  2.32  ...   \n",
       "1      0.82  0.75 -0.44  1.27 -0.69  0.93  2.69 -0.82 -2.23 -0.15  ...   \n",
       "2      0.08  0.96 -0.65  2.08  2.89  1.28  2.97  2.78  0.86  0.37  ...   \n",
       "3     -0.20  3.04  2.76  3.08  4.06  1.46  4.32  3.94  2.38  2.71  ...   \n",
       "4     -1.45 -0.22  1.05 -0.21  0.23 -1.56  1.22 -0.01 -0.15  1.64  ...   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "23329 -0.47  0.84  1.34  3.26  4.20  5.19  3.83  5.37  4.05  2.13  ...   \n",
       "23330  1.29  0.19  0.84 -0.43 -0.26  1.25  2.13  4.64  3.23  5.28  ...   \n",
       "23331  0.64 -0.31  0.64  1.00  0.09  1.93  3.16  3.17  3.52  3.45  ...   \n",
       "23332 -0.01 -0.16 -0.88 -0.10  0.79  1.35  2.03  1.44  3.79  4.40  ...   \n",
       "23333 -1.56  1.00  2.32  1.54  3.73  2.28  4.28  5.19  3.44  5.23  ...   \n",
       "\n",
       "       alea92  alea93  alea94  alea95  alea96  alea97  alea98  alea99  \\\n",
       "0        0.63    0.15    0.68    0.23    0.28    0.19    0.17    0.33   \n",
       "1        0.07    0.72    0.26    0.56    0.93    0.09    0.84    0.38   \n",
       "2        0.19    0.83    0.23    0.48    0.72    0.01    0.76    0.00   \n",
       "3        0.63    0.80    0.31    0.42    0.18    0.07    0.68    0.43   \n",
       "4        0.88    0.24    0.77    0.27    0.85    0.08    0.07    0.17   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "23329    0.83    0.78    0.29    0.95    0.34    0.51    0.89    0.79   \n",
       "23330    0.75    0.91    0.45    0.77    0.92    0.58    0.25    0.96   \n",
       "23331    0.28    0.22    0.72    0.22    0.70    0.69    0.02    0.63   \n",
       "23332    0.90    0.76    0.68    0.80    0.93    0.90    0.28    0.23   \n",
       "23333    0.95    0.05    0.12    0.12    0.13    0.41    0.10    0.15   \n",
       "\n",
       "       alea100  classe  \n",
       "0         0.43       0  \n",
       "1         0.49       0  \n",
       "2         0.37       0  \n",
       "3         0.37       0  \n",
       "4         0.93       0  \n",
       "...        ...     ...  \n",
       "23329     0.96       1  \n",
       "23330     0.56       1  \n",
       "23331     0.05       1  \n",
       "23332     0.16       1  \n",
       "23333     0.12       1  \n",
       "\n",
       "[23334 rows x 122 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c367bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns = ['classe'])\n",
    "y_train = train['classe']\n",
    "\n",
    "X_test = test.drop(columns = ['classe'])\n",
    "y_test = test['classe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "606d0bbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Combine the datasets\n",
    "# train['Dataset'] = 'Train'\n",
    "# test['Dataset'] = 'Test'\n",
    "\n",
    "# # Combine the datasets\n",
    "# combined_df = pd.concat([train, test])\n",
    "\n",
    "# # Calculate proportions for each class in each dataset\n",
    "# proportions_df = combined_df.groupby(['Dataset', 'classe']).size().reset_index(name='Count')\n",
    "\n",
    "# # Calculate the total number of samples in each dataset\n",
    "# total_counts = combined_df['Dataset'].value_counts().reset_index()\n",
    "# total_counts.columns = ['Dataset', 'Total']\n",
    "\n",
    "# # Merge to get the total counts into proportions_df\n",
    "# proportions_df = proportions_df.merge(total_counts, on='Dataset')\n",
    "\n",
    "# # Calculate the proportion\n",
    "# proportions_df['Proportion'] = proportions_df['Count'] / proportions_df['Total']\n",
    "\n",
    "\n",
    "# # Plot using seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.barplot(x='Dataset', y='Proportion', hue='classe', data=proportions_df)\n",
    "# plt.title('Proportion of Binary Classes in Training and Test Sets')\n",
    "# plt.ylabel('Proportion')\n",
    "# plt.xlabel('Dataset')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28afc5",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f28f8",
   "metadata": {},
   "source": [
    "#### Forward stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f76c8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['v4' 'v6' 'v8' 'v9' 'v10' 'v11' 'v12' 'v13' 'v15' 'v16' 'v17' 'v18' 'v21'\n",
      " 'alea1' 'alea7' 'alea14' 'alea70' 'alea73' 'alea80' 'alea85']\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "sfs = SequentialFeatureSelector(lr,\n",
    "                                n_features_to_select= 20,\n",
    "                                direction = 'forward',\n",
    "                                scoring='r2',\n",
    "                                cv=5)\n",
    "\n",
    "sfs = sfs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "selected_features =sfs.get_feature_names_out()\n",
    "print(f'Selected features: {selected_features}')\n",
    "\n",
    "\n",
    "X_train_selected = sfs.transform(X_train)\n",
    "X_test_selected = sfs.transform(X_test)\n",
    "lr.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3126fd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9190023142195938\n",
      "F1 score: 0.919908466819222\n"
     ]
    }
   ],
   "source": [
    "y_pred_bin = []\n",
    "threshold_p = 0.5\n",
    "for iloc, i in enumerate(y_pred):\n",
    "    if i >= threshold_p:\n",
    "        y_pred_bin.append(1)\n",
    "    else:\n",
    "        y_pred_bin.append(0)\n",
    "acc = accuracy_score(y_test, y_pred_bin)\n",
    "f1 = f1_score(y_test, y_pred_bin)\n",
    "print(f'Accuracy: {acc}')\n",
    "print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80894469",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f59db7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.0024757917332880134\n"
     ]
    }
   ],
   "source": [
    "# Use LassoCV to find the best alpha\n",
    "lasso_cv = LassoCV(cv=5, random_state=42)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Best alpha\n",
    "best_alpha = lasso_cv.alpha_\n",
    "print(f'Best alpha: {best_alpha}')\n",
    "\n",
    "# Train the Lasso model with the best alpha\n",
    "lasso_best = Lasso(alpha=best_alpha)\n",
    "lasso_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = lasso_best.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c95bcd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9223450758549756\n",
      "F1 score: 0.9231943031536114\n"
     ]
    }
   ],
   "source": [
    "y_pred_bin = []\n",
    "threshold_p = 0.5\n",
    "for iloc, i in enumerate(y_pred):\n",
    "    if i >= threshold_p:\n",
    "        y_pred_bin.append(1)\n",
    "    else:\n",
    "        y_pred_bin.append(0)\n",
    "acc = accuracy_score(y_test, y_pred_bin)\n",
    "f1 = f1_score(y_test, y_pred_bin)\n",
    "print(f'Accuracy: {acc}')\n",
    "print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48668270",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.         -0.         -0.00407755 -0.019093   -0.02221424 -0.00362407\n",
      "  0.          0.02221435  0.04218051  0.04751999  0.0662758   0.03872478\n",
      "  0.01480306 -0.00466215 -0.02392512 -0.03556203 -0.04361003 -0.03337441\n",
      " -0.01852327 -0.00798716  0.0039877   0.         -0.          0.\n",
      " -0.         -0.          0.          0.         -0.          0.\n",
      "  0.         -0.          0.         -0.          0.         -0.\n",
      " -0.         -0.         -0.          0.         -0.         -0.\n",
      "  0.         -0.          0.         -0.          0.          0.\n",
      " -0.          0.          0.          0.          0.         -0.\n",
      " -0.          0.         -0.          0.         -0.          0.\n",
      "  0.          0.         -0.          0.         -0.          0.\n",
      " -0.          0.         -0.         -0.          0.         -0.\n",
      " -0.         -0.         -0.          0.          0.          0.\n",
      " -0.         -0.         -0.          0.         -0.          0.\n",
      "  0.         -0.         -0.          0.         -0.         -0.\n",
      " -0.         -0.          0.          0.          0.         -0.\n",
      " -0.          0.         -0.          0.         -0.          0.\n",
      " -0.          0.         -0.          0.         -0.          0.\n",
      " -0.         -0.          0.         -0.          0.         -0.\n",
      " -0.          0.         -0.          0.         -0.         -0.\n",
      " -0.        ]\n",
      "Selected features: [ 2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    }
   ],
   "source": [
    "coefficients = lasso_best.coef_\n",
    "print(f'Coefficients: {coefficients}')\n",
    "\n",
    "selected_features = np.where(coefficients != 0)[0]\n",
    "print(f'Selected features: {selected_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae578c1e",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf1897b",
   "metadata": {},
   "source": [
    "#### lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ab0862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 0.012742749857031334\n",
      "Best cross-validation accuracy: 0.9189999999999999\n"
     ]
    }
   ],
   "source": [
    "# Initialize the logistic regression model with L1 penalty\n",
    "log_reg_l1 = LogisticRegression(penalty='l1', solver='saga', max_iter=10000)\n",
    "\n",
    "# Define the hyperparameter grid (C values for regularization strength)\n",
    "param_grid = {'C': np.logspace(-4, 4, 20)}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameter\n",
    "grid_search = GridSearchCV(log_reg_l1, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameter (C) and the corresponding score\n",
    "best_C = grid_search.best_params_['C']\n",
    "best_score = grid_search.best_score_\n",
    "print(f'Best C: {best_C}')\n",
    "print(f'Best cross-validation accuracy: {best_score}')\n",
    "\n",
    "# Train the logistic regression model with the best C\n",
    "log_reg_l1_best = LogisticRegression(penalty='l1', solver='saga', C=best_C, max_iter=10000)\n",
    "log_reg_l1_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg_l1_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66d1fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9236736093254478\n",
      "F1 score: 0.9241320553780618\n",
      "Confusion Matrix:\n",
      "[[10706  1054]\n",
      " [  727 10847]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 score: {f1}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3392a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [[ 0.          0.          0.         -0.1610363  -0.1799845   0.\n",
      "   0.          0.24877152  0.47625845  0.53328549  0.74095204  0.42689783\n",
      "   0.08099083 -0.02669093 -0.29672684 -0.3928026  -0.49827554 -0.3493908\n",
      "  -0.18771565 -0.03964553  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.        ]]\n",
      "Selected features: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "coefficients = log_reg_l1_best.coef_\n",
    "print(f'Coefficients: {coefficients}')\n",
    "\n",
    "selected_features = np.where(coefficients != 0)[0]\n",
    "print(f'Selected features: {selected_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5fea67",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36113164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "9000 fits failed out of a total of 45000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9000 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_leaf_nodes' parameter of RandomForestClassifier must be an int in the range [2, inf) or None. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1052: UserWarning: One or more of the test scores are non-finite: [0.9047 0.9099 0.9122 ... 0.8956 0.8952 0.8956]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'bootstrap': False, 'max_depth': 20, 'max_leaf_nodes': None, 'min_samples_leaf': 2, 'min_samples_split': 7, 'n_estimators': 200}\n",
      "Best cross-validation score: 0.9146000000000001\n",
      "Test accuracy: 0.9149738578897746\n",
      "F1 score: 0.9171884130561817\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100,150,200, 300,500],\n",
    "    'max_depth': [None,5, 10, 20, 30],\n",
    "    'min_samples_split': [2, 3, 5, 7,10],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 10],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_leaf_nodes': [None, 1, 3, 5,10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f'Best parameters: {best_params}')\n",
    "print(f'Best cross-validation score: {best_score}')\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test accuracy: {test_accuracy}')\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24bf892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature  Importance\n",
      "10     v11    0.136867\n",
      "16     v17    0.100616\n",
      "9      v10    0.093141\n",
      "8       v9    0.084035\n",
      "15     v16    0.082053\n",
      "14     v15    0.058759\n",
      "17     v18    0.049890\n",
      "11     v12    0.040993\n",
      "7       v8    0.034609\n",
      "18     v19    0.020886\n",
      "13     v14    0.016772\n",
      "6       v7    0.015056\n",
      "12     v13    0.012063\n",
      "4       v5    0.011388\n",
      "5       v6    0.009130\n",
      "3       v4    0.008706\n",
      "19     v20    0.006550\n",
      "2       v3    0.004676\n",
      "1       v2    0.002642\n",
      "49  alea29    0.002404\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# # Assuming your feature names are 'feature_1', 'feature_2', ..., 'feature_n'\n",
    "feature_names = [f'{i}' for i in X_train.columns]\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order and select the top 20\n",
    "top_20_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(20)\n",
    "\n",
    "print(top_20_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "818b13bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_0',\n",
       " 'feature_1',\n",
       " 'feature_2',\n",
       " 'feature_3',\n",
       " 'feature_4',\n",
       " 'feature_5',\n",
       " 'feature_6',\n",
       " 'feature_7',\n",
       " 'feature_8',\n",
       " 'feature_9',\n",
       " 'feature_10',\n",
       " 'feature_11',\n",
       " 'feature_12',\n",
       " 'feature_13',\n",
       " 'feature_14',\n",
       " 'feature_15',\n",
       " 'feature_16',\n",
       " 'feature_17',\n",
       " 'feature_18',\n",
       " 'feature_19',\n",
       " 'feature_20',\n",
       " 'feature_21',\n",
       " 'feature_22',\n",
       " 'feature_23',\n",
       " 'feature_24',\n",
       " 'feature_25',\n",
       " 'feature_26',\n",
       " 'feature_27',\n",
       " 'feature_28',\n",
       " 'feature_29',\n",
       " 'feature_30',\n",
       " 'feature_31',\n",
       " 'feature_32',\n",
       " 'feature_33',\n",
       " 'feature_34',\n",
       " 'feature_35',\n",
       " 'feature_36',\n",
       " 'feature_37',\n",
       " 'feature_38',\n",
       " 'feature_39',\n",
       " 'feature_40',\n",
       " 'feature_41',\n",
       " 'feature_42',\n",
       " 'feature_43',\n",
       " 'feature_44',\n",
       " 'feature_45',\n",
       " 'feature_46',\n",
       " 'feature_47',\n",
       " 'feature_48',\n",
       " 'feature_49',\n",
       " 'feature_50',\n",
       " 'feature_51',\n",
       " 'feature_52',\n",
       " 'feature_53',\n",
       " 'feature_54',\n",
       " 'feature_55',\n",
       " 'feature_56',\n",
       " 'feature_57',\n",
       " 'feature_58',\n",
       " 'feature_59',\n",
       " 'feature_60',\n",
       " 'feature_61',\n",
       " 'feature_62',\n",
       " 'feature_63',\n",
       " 'feature_64',\n",
       " 'feature_65',\n",
       " 'feature_66',\n",
       " 'feature_67',\n",
       " 'feature_68',\n",
       " 'feature_69',\n",
       " 'feature_70',\n",
       " 'feature_71',\n",
       " 'feature_72',\n",
       " 'feature_73',\n",
       " 'feature_74',\n",
       " 'feature_75',\n",
       " 'feature_76',\n",
       " 'feature_77',\n",
       " 'feature_78',\n",
       " 'feature_79',\n",
       " 'feature_80',\n",
       " 'feature_81',\n",
       " 'feature_82',\n",
       " 'feature_83',\n",
       " 'feature_84',\n",
       " 'feature_85',\n",
       " 'feature_86',\n",
       " 'feature_87',\n",
       " 'feature_88',\n",
       " 'feature_89',\n",
       " 'feature_90',\n",
       " 'feature_91',\n",
       " 'feature_92',\n",
       " 'feature_93',\n",
       " 'feature_94',\n",
       " 'feature_95',\n",
       " 'feature_96',\n",
       " 'feature_97',\n",
       " 'feature_98',\n",
       " 'feature_99',\n",
       " 'feature_100',\n",
       " 'feature_101',\n",
       " 'feature_102',\n",
       " 'feature_103',\n",
       " 'feature_104',\n",
       " 'feature_105',\n",
       " 'feature_106',\n",
       " 'feature_107',\n",
       " 'feature_108',\n",
       " 'feature_109',\n",
       " 'feature_110',\n",
       " 'feature_111',\n",
       " 'feature_112',\n",
       " 'feature_113',\n",
       " 'feature_114',\n",
       " 'feature_115',\n",
       " 'feature_116',\n",
       " 'feature_117',\n",
       " 'feature_118',\n",
       " 'feature_119',\n",
       " 'feature_120']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n",
    "-1 -21"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
